# PR Reviewer Assignment Service

## Оглавление

- [Описание проекта](#описание-проекта)
- [Инструкция по запуску и тестированию](#инструкция-по-запуску-и-тестированию)
  - [Запуск](#запуск)
  - [Тесты](#тесты)
- [Архитектура сервиса и разделение ответственности](#архитектура-сервиса-и-разделение-ответственности)
  - [Handlers](#1-handlers)
  - [Services](#2-services-слой-бизнес-логики)
  - [Repository](#3-repository-работа-с-бд)
  - [Database и миграции](#4-database-и-миграции)
- [Основные проблемы и решения](#основные-проблемы-и-решения)
  1. [Структурное решение и интерфейсы](#1-структурное-решение-и-интерфейсы)
  2. [Корректный возврат ошибок](#2-корректный-возврат-ошибок-и-соответствие-api)
  3. [Отдельная база для тестов](#3-отдельная-база-для-тестов)
  4. [Линтинг и качество кода](#4-линтинг-и-качество-кода)
  5. [Подъём миграций внутри контейнера](#5-подъём-миграций-внутри-контейнера)
  6. [Роутер и выбор фреймворка](#6-роутер-и-выбор-фреймворка)
- [Основные команды для проверки работы](#основные-команды-для-проверки-работы)
- [Результаты нагрузочного тестирования](#результаты-нагрузочного-тестирования)
- [Дополнительно](#дополнительно)

## Описание проекта

Сервис назначения ревьюеров для Pull Requestов реализован на Go с использованием PostgreSQL. Сервис обеспечивает управление командами и пользователями, автоматическое назначение ревьюверов, переназначение при необходимости, а также сбор статистики по PR и активности пользователей.

Cервис поднимается через Docker Compose и взаимодействует с клиентом через приведенный API (`openapi.yml`).

## Инструкция по запуску и тестированию

### Запуск

1. Поднять сервисы и БД через Docker Compose make-командами:
   ```
   docker-compose up -d
   ```
2. Поднять миграции (при первом запуске)
   ```
    make migrate-up
   ```
2. Проверить работу сервиса через API (например, `curl` или postman).

### Тесты
Интеграционные тесты используют отдельную тестовую базу (`TEST_DATABASE_URL`), очищаемую перед каждым запуском. Запуск тестов:
   ```
   make test
   ```

## Архитектура сервиса и разделение ответственности

Сервис построен по принципу многослойной архитектуры (репозиторий - сервис - хэндлер):

### 1. Handlers
- Отвечают за взаимодействие с клиентами в соответствии с описанным API (`openapi.yml`).
- Валидируют входные данные и формируют HTTP-ответы.
- Логируют ошибки и ключевые события.

#### Примеры созданных эндпоинтов:
- `/users/setIsActive` - обновление активности пользователя
- `/users/getReview` - получение PR, где пользователь назначен ревьюером
- `/team/add` - создание команды
- `/pullRequest/create` - создание PR
- `/stats` - сбор статистики

### 2. Services (слой бизнес-логики)
- Инкапсулируют правила работы сервиса.
- Обеспечивают назначение ревьюверов с проверкой активности и исключением автора.
- Выполняют переназначение ревьюверов при деактивации пользователей команды.

#### Основные сервисы:
- `UserService` - управление пользователями, получение PR для ревью, массовая деактивация команды.
- `TeamService` - управление командами, создание и проверка существования, сбор статистики.
- `PullRequestService` - создание, merge, переназначение ревьюверов, сбор статистики по PR.

### 3. Repository (работа с БД)
- Реализует и исполняет SQL-запросы (хранятся в `/repository/queries/sql_queries.go`) и транзакции.
- Отвечает за атомарность операций, целостность данных и соблюдение некоторых специфичных бизнес-правил.

#### Примеры репозиториев:
- `UserRepo` - CRUD операций, выборка активных пользователей, получение PR для ревью.
- `TeamRepo` - создание команд с пользователями, выборка участников, проверка существования.
- `PullRequestRepo` - создание и merge PR, назначение и обновление ревьюверов, выборка PR по команде.

### 4. Database и миграции
- PostgreSQL с миграциями, хранящимися в `migrations/`.
- Миграции гарантируют одинаковую структуру БД для разработки, тестов и продакшена.

### **Структура базы данных**
- Таблица `users` - `user_id`, `username`, `team_name`, `is_active`.
- Таблица `teams` - `team_name`.
- Таблица `pull_requests` - `pull_request_id`, `pull_request_name`, `author_id`, `status`, `created_at`, `merged_at`.
- Таблица `pull_request_reviewers` - связь `PR` - `User` (многие-ко-многим).

#### Связи:
- (`users` - `teams`) - многие к одному
- (`pull_requests` - `users`(авторы)) - многие к одному
- (`pull_requests` - `teams`(ревьюеры)) - многие ко многим

#### Выбранная структура позволяет:
1. атомарно создавать команды с пользователями через транзакции.
2. легко получать список ревьюверов PR, активных участников команды и делать переназначения.

## Основные проблемы и решения

### 1. Структурное решение и интерфейсы

#### **Проблема:**
Планировалось построить архитектуру с чётким разделением слоёв: Handlers - Service - Repository - DB. Мне нужно было сделать так, чтобы слои могли общаться через абстракции и при этом минимизировать жесткую связку к конкретной реализации БД.

#### **Выбранное решение**
Для этого я ввёл интерфейсы `UserReader` и `UserWriter`. `UserReader` - это слой только для чтения данных (получение пользователя, списка активных участников команды, PR, где он ревьюер), `UserWriter` - только для изменения (создание, установка активности). В итоге `UserRepo` реализует оба интерфейса, а сервисы могут работать с ними отдельно, не завися от конкретного SQL или структуры таблиц. Аналогичные интерфейсы были определены и для других доменных сущностей.

#### **Реализация:**
Сервис `PullRequestService` принимает `UserReader`, чтобы получать активных участников команды для назначения ревьюверов. При этом он ничего не знает о том, как устроена таблица users, какие поля есть или как строятся SQL-запросы. Такой подход позволил горизонтально развязать слои: сервисы зависят только от контрактов, а репозитории скрывают детали реализации.

#### **Как это повлияло на сервис:**
- Упрощена потенциальная возможночсть unit-тестирования: для тестов можно было подменять интерфейсы моками.
- Чистота кода: сервисы не делают SQL-запросы напрямую, нет смешения бизнес-логики с БД.
- Масштабируемость: легко добавить кеш или другой источник данных, не меняя сервис.

### 2. Корректный возврат ошибок и соответствие API

#### **Проблема:**
Хотелось, чтобы сервис возвращал понятные HTTP-ошибки, но при этом не раскрывал внутренние SQL или стек-трейсы клиенту.

#### **Решение:**
- В репозиториях возвращаем внутренние ошибки, например `ErrPRNotFound`, `ErrTeamExists`.
- В сервисах ловим эти ошибки и конвертируем их в понятные коды предусмотренные в API: `404 NOT_FOUND`, `409 CONFLICT`.
- Хэндлеры используют эти коды и формируют структурированный JSON.

#### **Почему выбран такой подход и его результат:**
Так клиент получает чёткий контракт API, сервисы остаются корректными и не перегруженными, а логирование ошибок происходит через `zap.SugaredLogger`. Любая непредвиденная ошибка логируется, но клиенту возвращается общий код 500.

### 3. Отдельная база для тестов

#### **Проблема:**
Интеграционные тесты не должны ломать данные в основной БД.

#### **Решение:**
Отдельная тестовая база `pr_service_test`, полностью независимая.

#### **Реализация:**
- Подключение через переменную окружения `TEST_DATABASE_URL`.
- Перед каждым тестом вызывается `ResetDB()`, который делает `TRUNCATE` всех таблиц с `RESTART IDENTITY CASCADE`.
- Миграции применяются к тестовой базе через `migrate` внутри теста.

#### **Как это повлияло на сервис:**
- Можно запускать тесты параллельно без риска сломать рабочие данные.
- Тесты стали воспроизводимыми: одна и та же база, один и тот же набор данных, перед каждым тестом чистка.
- Проблемы с миграциями для тестовой базы решены через отдельный вызов `migrate.NewWithDatabaseInstance`.

### 4. Линтинг и качество кода

#### **Проблема:**
При линтинге было выявлено множество issues: пропущенные проверки ошибок, пустые блоки, комментарии для экспортируемых функций, форматирование и др..

#### **Решение:**
- Все проверки ошибок добавлены (`http.Post`, `resp.Body.Close`, транзакции SQL).
- Пустые блоки удалены или заполнены комментариями.
- Экспортируемые структуры и функции получили корректные комментарии в стиле GoDoc.
- Код форматирован через `gofmt`.

#### **Как это повлияло:**
- Код стал стабильнее и безопаснее.
- Проблемы с линтером пропали.

### 5. Подъём миграций внутри контейнера

#### **Проблема:**
При запуске через `docker-compose up` миграции не всегда успевали примениться, не применялись вовсе из-за ошибок `migrate`, сервис падал с ошибкой, что таблицы не найдены.

#### **Решение:**
- Установлены необходимые утилиты и пакеты на стадии запуска контейнера с приложением.
- Добавлен healthcheck на контейнер БД (`pg_isready`).
- В Makefile команды `migrate-up` ждут готовности БД перед запуском миграций через цикл `until pg_isready`.
- Внутри контейнера сервис ждёт готовности базы перед подключением.

#### **Как это повлияло на сервис**
- Сервис стабильно стартует даже при медленном поднятии контейнера БД.
- Можно спокойно запускать Docker Compose, миграции всегда применяются в правильной последовательности.

### 6. Роутер и выбор фреймворка

#### **Проблема:**
Необходимо выбрать фреймворк для маршрутизации, который был бы одновременно простым, удобным для работы с JSON, легко интегрируемым с логированием, и при этом не не замедлял сервис.

#### **Решения:**
Было рассмотрено несколько вариантов. Стандартный `net/http` обладает высокой производительностью, но для приведенного API необходимо бы было вручную парсить JSON, обрабатывать ошибки и регистрировать маршруты, что сделало бы код громоздким. `Gorilla Mux` предоставляет удобную поддержку динамических роутов, а работа с JSON требовала дополнительного кода. `Echo` показался хорошим, но вариантом избыточным для такого относительно небольшого сервиса. В качестве фреймворка был выбран `Gin`. Он минималистичный, быстрый, удобно работает с JSON и хорошо интегрируется с логированием.

#### **Реализация:**
Все маршруты собраны в функции `NewRouter`, где определены пути для пользователей, команд, PR и статистики. Обработка ошибок и успешных ответов выполняется в обработчиках, что делает код читаемым и поддерживаемым.

#### **Как это повлияло на сервис:**
Использование `Gin` упростило интеграционные тесты и работу с JSON, обеспечило ясную структуру маршрутов и обработчиков, а также позволило легко расширять сервис без перегрузки кода дополнительными обертками.


## Основные команды для проверки работы

- `make build` - сборка Docker образов
- `make rebuild` - полная пересборка с откатом миграций
- `make up` - запуск сервисов
- `make migrate-up` - применение миграций
- `make test` - запуск интеграционных тестов
- `make logs` - просмотр логов

## Результаты нагрузочного тестирования
Нагрузочное тестирование проводилось с помощью bash-скриптов `hard_test.sh` и `api_test.sh`.

Первый файл (`./tests/hard_test.sh`) - нагрузочный тест с возомжностью указания параметров. Аналогичным образом посылаются запросы. Скрипт анализирует время их обработки, статус-коды и ответы и выводит статистику по всем эндпоинтам. После его выполнения рекомендуется обновлять миграции, так как файл работает с основной базой данных. Проведенное с его помощью тестирование показало, что сервис соответствует требованиям. Для примера в папке `./logs` приведены логи выполнения данного скрипта. Результат нагрузочного тестирования приведен ниже:
    ```
    === запуск нагрузочного теста на http://localhost:8080 ===

    === резюме нагрузочного теста ===
    Всего запросов:      831
    Среднее время отклика: 48.8183 ms
    Максимальное время отклика: 828 ms
    Минимальное время отклика: 15 ms
    Процент успешных запросов: 100.000%
    Количество неуспешных запросов: 0

    === sli по endpoint ===
    pullRequest/reassign: avg_time=62.62ms, success=100.00% (120/120)
    team/get: avg_time=30.70ms, success=100.00% (20/20)
    pullRequest/merge: avg_time=55.05ms, success=100.00% (60/60)
    pullRequest/create: avg_time=55.10ms, success=100.00% (120/120)
    users/getReview: avg_time=40.43ms, success=100.00% (249/249)
    team/deactivate: avg_time=40.00ms, success=100.00% (20/20)
    health: avg_time=49.00ms, success=100.00% (1/1)
    users/setIsActive: avg_time=44.28ms, success=100.00% (200/200)
    team/add: avg_time=65.50ms, success=100.00% (40/40)
    stats: avg_time=131.00ms, success=100.00% (1/1)
    ```

Второй файл (`./tests/api_test.sh`) - облегченная проверка соответствия основных эндпоинтов ключевым положения используемого API. Данный файл отправляет различные запросы и выводит полученные ответы в консоль. После его выполнения рекомендуется обновлять миграции, так как файл работает с основной базой данных. Полученный после исполнения скрипта результат соответствует всем требованиям и приведен в файле `./logs/result.txt`

## Дополнительно

- Реализовано интеграционное тестирование
- Добавлен метод массовой деактивации пользователей команды и безопасная переназначаемость открытых PR
- Добавлена возможность расширения статистики и мониторинга за счет созданного эндпоинта `/stats`
- Описана конфигурация линтера
- Проведено нагрузочное тестирование с помощью скрипта на языке bash

